# -*- coding: utf-8 -*-
"""linear+MAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVTD97L6ZX0Lz6QDiXFk9pw9w-TNU3YK
"""

import torch as t
import numpy as np
import matplotlib.pyplot as plt

w = 0.3
b = 0.9

X = t.arange(0,1,0.01).unsqueeze(dim=1)
Y = w*X + b

train_slip = int(0.8*len(X))
x_train, y_train = X[:train_slip], Y[:train_slip]
x_test, y_test = X[train_slip:], Y[train_slip:]

def plot_pre(train_data=x_train,
             train_labels=y_train,
             test_data=x_test,
             test_labels=y_test,
             predictions=None):
  """
  plots pre
  """
  plt.figure(figsize=(10,7))
  plt.scatter(train_data, train_labels, c="b", s=7, label="Trainning")
  plt.scatter(test_data, test_labels, c="b", s=7, label="Testing")

  if predictions is not None:
    plt.scatter(test_data, predictions, c="r", s=7, label="Predictions")

  plt.legend(prop={"size": 15})

import torch.nn as nn
class linear(nn.Module):
  def __init__(self):
    super().__init__()
    self.w = nn.Parameter(t.randn(1,
                                   requires_grad=True,
                                   dtype=t.float))
    self.b = nn.Parameter(t.randn(1,
                                   requires_grad=True,
                                   dtype=t.float))
  def forward(self, x:t.Tensor) -> t.Tensor:
    return self.w*x + self.b

t.manual_seed(20)
model = linear()

with t.inference_mode():
  y_preds = model(x_test)

plot_pre(predictions=y_preds)

lossfc = nn.L1Loss()
op = t.optim.SGD(params=model.parameters(),
                 lr=0.01)

epochs = 1000
epoch_count = []
train_loss_list = []
test_loss_list = []
for epoch in range(epochs):
  model.train()
  y_pred = model(x_train)
  loss = lossfc(y_pred, y_train)
  op.zero_grad()
  loss.backward()
  op.step()
  model.eval()
  with t.inference_mode():
    test_pred = model(x_test)
    test_loss = lossfc(test_pred, y_test.type(t.float))

  if epoch % 10 == 0:
    epoch_count.append(epoch)
    train_loss_list.append(loss.detach().numpy())
    test_loss_list.append(test_loss.detach().numpy())

with t.inference_mode():
    y_preds = model(x_test)

plt.plot(epoch_count, train_loss_list, label="Train loss")
plt.plot(epoch_count, test_loss_list, label="Test loss")
plt.title("Training and test loss curves")

lossfc = nn.L1Loss()
op = t.optim.SGD(params=model.parameters(),
                 lr=0.001)

epochs = 100
epoch_count = []
train_loss_list = []
test_loss_list = []
for epoch in range(epochs):
  model.train()
  y_pred = model(x_train)
  loss = lossfc(y_pred, y_train)
  op.zero_grad()
  loss.backward()
  op.step()
  model.eval()
  with t.inference_mode():
    test_pred = model(x_test)
    test_loss = lossfc(test_pred, y_test.type(t.float))

  if epoch % 10 == 0:
    epoch_count.append(epoch)
    train_loss_list.append(loss.detach().numpy())
    test_loss_list.append(test_loss.detach().numpy())

plt.plot(epoch_count, train_loss_list, label="Train loss")
plt.plot(epoch_count, test_loss_list, label="Test loss")
plt.title("Training and test loss curves")

lossfc = nn.L1Loss()
op = t.optim.SGD(params=model.parameters(),
                 lr=0.0001)

epochs = 1000
epoch_count = []
train_loss_list = []
test_loss_list = []
for epoch in range(epochs):
  #đưa ra dự đoán
  model.train()
  y_pred = model(x_train)
  #tính loss
  loss = lossfc(y_pred, y_train)
  #reset grad vì grad bị tích lũy
  op.zero_grad()
  #bắt đầu cải tiến
  #tính toán loss cho từng tham số của từng neutron với thuộc tính requires_grad=True
  loss.backward()
  #cải tiến, cập nhập w và bias thông qua thuật toán tối ưu cụ thể và grad ở backward
  op.step()
  model.eval()
  with t.inference_mode():
    test_pred = model(x_test)
    test_loss = lossfc(test_pred, y_test.type(t.float))

  if epoch % 10 == 0:
    epoch_count.append(epoch)
    train_loss_list.append(loss.detach().numpy())
    test_loss_list.append(test_loss.detach().numpy())

plt.plot(epoch_count, train_loss_list, label="Train loss")
plt.plot(epoch_count, test_loss_list, label="Test loss")
plt.title("Training and test loss curves")

lossfc = nn.L1Loss()
op = t.optim.SGD(params=model.parameters(),
                 lr=0.00001)
epochs = 100
epoch_count = []
train_loss_list = []
test_loss_list = []
for epoch in range(epochs):
  model.train()
  y_pred = model(x_train)
  loss = lossfc(y_pred, y_train)
  op.zero_grad()
  loss.backward()
  op.step()
  model.eval()
  with t.inference_mode():
    test_pred = model(x_test)
    test_loss = lossfc(test_pred, y_test.type(t.float))

  if epoch % 10 == 0:
    epoch_count.append(epoch)
    train_loss_list.append(loss.detach().numpy())
    test_loss_list.append(test_loss.detach().numpy())
plt.plot(epoch_count, train_loss_list, label="Train loss")
plt.plot(epoch_count, test_loss_list, label="Test loss")
plt.title("Training and test loss curves")

plot_pre(predictions=y_preds)

from pprint import pprint
print("The model learned the following values for weights and bias:")
pprint(model.state_dict())
print("\nAnd the original values for weights and bias are:")
print(f"weights: {w}, bias: {b}")